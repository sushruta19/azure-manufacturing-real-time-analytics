{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1614171c-0c96-4323-8880-603cbbc35a97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 03_gold_spc_charts.py\n",
    "#\n",
    "# SPC Charts (P-chart, Xbar & R)\n",
    "#\n",
    "\n",
    "import dlt\n",
    "from pyspark.sql.functions import (\n",
    "    col, avg, sum as spark_sum, lit, round as spark_round,\n",
    "    row_number, sqrt, max as spark_max, min as spark_min, when,\n",
    "    unix_timestamp, from_unixtime, to_timestamp\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# CONFIG\n",
    "# -----------------------------------------------------------\n",
    "SAMPLE_SIZE_DEFECT = 30   # for P-chart (subgroup size)\n",
    "SAMPLE_SIZE_TEMP = 10     # for Xbar-R chart (subgroup size)\n",
    "\n",
    "# ===========================================================\n",
    "# 1️⃣ P-CHART (Per Assembly Line)\n",
    "# ===========================================================\n",
    "@dlt.table(\n",
    "    name=\"03_gold.inspector_pchart\",\n",
    "    comment=\"P-chart per assembly line showing defect proportion, control limits and subgroup time\"\n",
    ")\n",
    "def inspector_pchart():\n",
    "    df = dlt.read(\"02_silver.inspector_enriched\")\n",
    "\n",
    "    # Order each line's events chronologically and assign row/subgroup ids\n",
    "    w = Window.partitionBy(\"line_id\").orderBy(\"event_time\")\n",
    "    df = df.withColumn(\"row_id\", row_number().over(w))\n",
    "    df = df.withColumn(\"subgroup_id\", ((col(\"row_id\") - 1) / SAMPLE_SIZE_DEFECT).cast(\"int\"))\n",
    "\n",
    "    # Aggregate per subgroup (fixed sample size per line).\n",
    "    # For subgroup_time we average unix timestamps then convert back to timestamp.\n",
    "    df_grouped = (\n",
    "        df.groupBy(\"line_id\", \"subgroup_id\")\n",
    "          .agg(\n",
    "              spark_sum(\"defective_count\").alias(\"total_defective\"),\n",
    "              spark_sum(\"produced_count\").alias(\"total_produced\"),\n",
    "              avg(unix_timestamp(col(\"event_time\"))).alias(\"avg_event_epoch\")\n",
    "          )\n",
    "          .withColumn(\"p_i\", when(col(\"total_produced\") > 0, col(\"total_defective\") / col(\"total_produced\")).otherwise(None))\n",
    "          # convert averaged epoch back to timestamp\n",
    "          .withColumn(\"subgroup_time\", to_timestamp(from_unixtime(col(\"avg_event_epoch\"))))\n",
    "    )\n",
    "\n",
    "    # Compute p_bar per line (overall proportion across all subgroups)\n",
    "    stats = (\n",
    "        df_grouped.groupBy(\"line_id\")\n",
    "                  .agg(\n",
    "                      (spark_sum(\"total_defective\") / spark_sum(\"total_produced\")).alias(\"p_bar\")\n",
    "                  )\n",
    "    )\n",
    "\n",
    "    # Join to get p_bar per subgroup (per line) and compute limits\n",
    "    df_joined = df_grouped.join(stats, on=\"line_id\", how=\"left\")\n",
    "\n",
    "    df_final = (\n",
    "        df_joined.withColumn(\n",
    "            \"UCL\",\n",
    "            col(\"p_bar\") + 3 * sqrt((col(\"p_bar\") * (1 - col(\"p_bar\"))) / lit(SAMPLE_SIZE_DEFECT))\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"LCL\",\n",
    "            col(\"p_bar\") - 3 * sqrt((col(\"p_bar\") * (1 - col(\"p_bar\"))) / lit(SAMPLE_SIZE_DEFECT))\n",
    "        )\n",
    "        .withColumn(\"LCL\", when(col(\"LCL\") < 0, 0).otherwise(col(\"LCL\")))\n",
    "        .select(\n",
    "            \"line_id\",\n",
    "            \"subgroup_id\",\n",
    "            \"subgroup_time\",\n",
    "            spark_round(\"p_i\", 4).alias(\"p_i\"),\n",
    "            spark_round(\"p_bar\", 4).alias(\"p_bar\"),\n",
    "            spark_round(\"UCL\", 4).alias(\"UCL\"),\n",
    "            spark_round(\"LCL\", 4).alias(\"LCL\"),\n",
    "            \"total_defective\",\n",
    "            \"total_produced\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return df_final\n",
    "\n",
    "# ===========================================================\n",
    "# 2️⃣ XBAR-R CHART (Per Machine Type per Line)\n",
    "# ===========================================================\n",
    "def compute_xbar_r(df):\n",
    "    # Order by event_time within each (line_id, machine_type) and assign subgroup ids\n",
    "    w = Window.partitionBy(\"line_id\", \"machine_type\").orderBy(\"event_time\")\n",
    "    df = df.withColumn(\"row_id\", row_number().over(w))\n",
    "    df = df.withColumn(\"subgroup_id\", ((col(\"row_id\") - 1) / SAMPLE_SIZE_TEMP).cast(\"int\"))\n",
    "\n",
    "    # Compute subgroup stats and subgroup_time (avg of event_time)\n",
    "    subgroups = (\n",
    "        df.groupBy(\"line_id\", \"machine_type\", \"subgroup_id\")\n",
    "          .agg(\n",
    "              avg(\"temperature_c\").alias(\"xbar\"),\n",
    "              (spark_max(\"temperature_c\") - spark_min(\"temperature_c\")).alias(\"R\"),\n",
    "              avg(unix_timestamp(col(\"event_time\"))).alias(\"avg_event_epoch\")\n",
    "          )\n",
    "          .withColumn(\"subgroup_time\", to_timestamp(from_unixtime(col(\"avg_event_epoch\"))))\n",
    "    )\n",
    "\n",
    "    # Compute grand averages per (line, machine)\n",
    "    stats = (\n",
    "        subgroups.groupBy(\"line_id\", \"machine_type\")\n",
    "                 .agg(\n",
    "                     avg(\"xbar\").alias(\"xbarbar\"),\n",
    "                     avg(\"R\").alias(\"Rbar\")\n",
    "                 )\n",
    "    )\n",
    "\n",
    "    # Constants for n = SAMPLE_SIZE_TEMP (n=10 here)\n",
    "    # A2, D3, D4 values for n=10\n",
    "    A2, D3, D4 = 0.308, 0.223, 1.777\n",
    "\n",
    "    df_joined = subgroups.join(stats, on=[\"line_id\", \"machine_type\"], how=\"left\")\n",
    "\n",
    "    return (\n",
    "        df_joined.select(\n",
    "            \"line_id\",\n",
    "            \"machine_type\",\n",
    "            \"subgroup_id\",\n",
    "            \"subgroup_time\",\n",
    "            spark_round(\"xbar\", 3).alias(\"xbar\"),\n",
    "            spark_round(\"R\", 3).alias(\"R\"),\n",
    "            spark_round(\"xbarbar\", 3).alias(\"xbarbar\"),\n",
    "            spark_round(\"Rbar\", 3).alias(\"Rbar\"),\n",
    "            spark_round((col(\"xbarbar\") + A2 * col(\"Rbar\")), 3).alias(\"UCLx\"),\n",
    "            spark_round((col(\"xbarbar\") - A2 * col(\"Rbar\")), 3).alias(\"LCLx\"),\n",
    "            spark_round((D4 * col(\"Rbar\")), 3).alias(\"UCLr\"),\n",
    "            spark_round((D3 * col(\"Rbar\")), 3).alias(\"LCLr\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"03_gold.temperature_xbar_r_chart\",\n",
    "    comment=\"Xbar and R chart per machine type and assembly line (with subgroup_time)\"\n",
    ")\n",
    "def temperature_xbar_r_chart():\n",
    "    # union drillcutter and polisher (they have same normalized schema)\n",
    "    df = dlt.read(\"02_silver.drillcutter_enriched\").unionByName(\n",
    "             dlt.read(\"02_silver.polisher_enriched\")\n",
    "         )\n",
    "\n",
    "    return compute_xbar_r(df)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_Gold_SPC_Charts",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}